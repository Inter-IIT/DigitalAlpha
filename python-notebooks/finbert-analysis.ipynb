{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Availability:  True\n",
      "Current Device :  NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Available Devices\n",
      "0 ) Device Name:  NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Availability: \", torch.cuda.is_available())\n",
    "gpu_index = torch.cuda.current_device()\n",
    "print(\"Current Device : \", torch.cuda.get_device_name(gpu_index))\n",
    "print(\"Available Devices\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, \") Device Name: \", torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Testing Tensorflow-gpu\n",
    "\n",
    "- Language: markdown\n",
    "- Path: finbert-analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Import the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The Company’s financial instruments that are exposed to concentrations of credit risk consist primarily of cash, cash equivalents, restricted cash, available-for-sale securities, and accounts receivable.\"\n",
    "    \"Although the Company maintains cash deposits, cash equivalent balances, and available-for-sale securities with multiple financial institutions, the deposits, at times, may exceed federally insured limits.\",\n",
    "    \"Cash and cash equivalents may be withdrawn or redeemed on demand.\",\n",
    "    \"The Company believes that the financial institutions that hold its cash and cash equivalents and restricted cash are financially sound and, accordingly, minimal credit risk exists with respect to these balances.\",\n",
    "    \"The Company also maintains investments in U.S. treasury securities, U.S. government agency securities, commercial paper, and corporate bonds that carry high credit ratings and accordingly, minimal credit risk exists with respect to these balances.\",\n",
    "    \"Cash equivalents consist of money market funds, commercial paper, and corporate bonds which are invested through financial institutions in the United States.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Company’s financial instruments that are exposed to concentrations of credit risk consist primarily of cash, cash equivalents, restricted cash, available-for-sale securities, and accounts receivable.Although the Company maintains cash deposits, cash equivalent balances, and available-for-sale securities with multiple financial institutions, the deposits, at times, may exceed federally insured limits. ---- neutral\n",
      "Cash and cash equivalents may be withdrawn or redeemed on demand. ---- neutral\n",
      "The Company believes that the financial institutions that hold its cash and cash equivalents and restricted cash are financially sound and, accordingly, minimal credit risk exists with respect to these balances. ---- positive\n",
      "The Company also maintains investments in U.S. treasury securities, U.S. government agency securities, commercial paper, and corporate bonds that carry high credit ratings and accordingly, minimal credit risk exists with respect to these balances. ---- neutral\n",
      "Cash equivalents consist of money market funds, commercial paper, and corporate bonds which are invested through financial institutions in the United States. ---- neutral\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "outputs = finbert(**inputs)[0]\n",
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "for idx, sent in enumerate(sentences):\n",
    "    print(sent, '----', labels[np.argmax(outputs.detach().numpy()[idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now analysing the code bit by bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    3,     6,    37, 30748,    58,    39,   786,    15,    21,  2856,\n",
       "             9,  7632,     7,    97,   177,  1768,   106,     7,    50,   585,\n",
       "            50,  1499,   585,  1349,    50,   585,   213,    85,    14,    85,\n",
       "           282,    86,   585,     8,   491,  1179,    48,   488,     6,    37,\n",
       "          2705,    50,   976,   585,    50,  1764,  1344,   585,     8,   213,\n",
       "            85,    14,    85,   282,    86,    20,   533,    39,  1616,   585,\n",
       "             6,   976,   585,    28,  1101,   585,    32,  1441, 11574,  2256,\n",
       "          2574,    48,     4],\n",
       "        [    3,    50,     8,    50,  1499,    32,    25, 11461,    16,  6039,\n",
       "            19,   302,    48,     4,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [    3,     6,    37,   852,    15,     6,    39,  1616,    15,   871,\n",
       "            38,    50,     8,    50,  1499,     8,  1349,    50,    21,  7687,\n",
       "          4757,     8,   585,  7672,   585,  4004,    97,   177,  4804,    20,\n",
       "           522,     9,    43,  1344,    48,     4,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [    3,     6,    37,    67,  2705,   275,    10,  4729,    48,    58,\n",
       "            48,  2117,    86,   585,  4729,    48,    58,    48,   542,  1404,\n",
       "            86,   585,   256,  1659,   585,     8,   371,  2394,    15,  2093,\n",
       "           307,    97,   498,     8,  7672,   585,  4004,    97,   177,  4804,\n",
       "            20,   522,     9,    43,  1344,    48,     4,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [    3,    50,  1499,  1768,     7,  1247,    52,   543,   585,   256,\n",
       "          1659,   585,     8,   371,  2394,    31,    21,  1462,   115,    39,\n",
       "          1616,    10,     6,   245,   357,    48,     4,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.9878306, -3.2382145, -3.6300595], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = finbert(**inputs)\n",
    "y = outputs[0]\n",
    "y_sum = np.average(y.detach().numpy(),axis=0)\n",
    "y_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Company’s financial instruments that are exposed to concentrations of credit risk consist primarily of cash, cash equivalents, restricted cash, available-for-sale securities, and accounts receivable.Although the Company maintains cash deposits, cash equivalent balances, and available-for-sale securities with multiple financial institutions, the deposits, at times, may exceed federally insured limits.\n",
      "\n",
      "Sentiment:  neutral\n",
      "Sentiment Probability:  4.79147\n",
      "\n",
      " Cash and cash equivalents may be withdrawn or redeemed on demand.\n",
      "\n",
      "Sentiment:  neutral\n",
      "Sentiment Probability:  6.8653197\n",
      "\n",
      " The Company believes that the financial institutions that hold its cash and cash equivalents and restricted cash are financially sound and, accordingly, minimal credit risk exists with respect to these balances.\n",
      "\n",
      "Sentiment:  positive\n",
      "Sentiment Probability:  2.8210857\n",
      "\n",
      " The Company also maintains investments in U.S. treasury securities, U.S. government agency securities, commercial paper, and corporate bonds that carry high credit ratings and accordingly, minimal credit risk exists with respect to these balances.\n",
      "\n",
      "Sentiment:  neutral\n",
      "Sentiment Probability:  2.2388155\n",
      "\n",
      " Cash equivalents consist of money market funds, commercial paper, and corporate bonds which are invested through financial institutions in the United States.\n",
      "\n",
      "Sentiment:  neutral\n",
      "Sentiment Probability:  7.8977084\n",
      "\n",
      "Overall Sentiment:  neutral\n"
     ]
    }
   ],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "for i, sentence in enumerate(sentences):\n",
    "    #Iterate through every sentence\n",
    "    print(\"\\n\",sentence)\n",
    "    j = np.argmax(y.detach().numpy()[i])\n",
    "    print(\"\\nSentiment: \" , labels[j])\n",
    "    print(\"Sentiment Probability: \" , y.detach().numpy()[i][j])\n",
    "j_sum = np.argmax(y_sum)\n",
    "print(\"\\nOverall Sentiment: \" , labels[j_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_base = \"https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/\" # Base URL for filing\n",
    "filing_summary = file_base + \"FilingSummary.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FilingSummary'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as req\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "head = {\n",
    "    \"User-Agent\": \"Alpha-Explorer/1.0\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "res = req.get(filing_summary, headers=head)\n",
    "\n",
    "root = et.fromstring(res.text)\n",
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '2103102 - Disclosure - Summary of Significant Accounting Policies',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R10.htm'},\n",
       " {'name': '2124106 - Disclosure - Leases',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R14.htm'},\n",
       " {'name': '2129107 - Disclosure - Debt',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R15.htm'},\n",
       " {'name': '2135108 - Disclosure - Commitments and Contingencies',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R16.htm'},\n",
       " {'name': '2139109 - Disclosure - Common Stock',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R17.htm'},\n",
       " {'name': '2143110 - Disclosure - Stock-based Compensation',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R18.htm'},\n",
       " {'name': '2204201 - Disclosure - Summary of Significant Accounting Policies (Policies)',\n",
       "  'url': 'https://www.sec.gov/Archives/edgar/data/1477333/000147733322000008/R24.htm'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_dict = [] # Array to store our dictionary\n",
    "trigger_one = ['DISCLOSURE']\n",
    "trigger_list = ['ACCOUNTING', 'LEASES', 'DEBT','COMMITMENTS','STOCK',] # List of triggers to look for\n",
    "for report in root.iter('Report'):\n",
    "    dict = {}\n",
    "    for trigger_word in trigger_list:\n",
    "        name = report.find('LongName').text\n",
    "        if (trigger_word.lower() in name.lower() and 'disclosure' in name.lower()):\n",
    "            if (\"table\" in name.lower() or \"details\" in name.lower()):\n",
    "                continue\n",
    "            try:\n",
    "                dict[\"name\"] = report.find('LongName').text\n",
    "                dict[\"url\"] = file_base+report.find('HtmlFileName').text\n",
    "                component_dict.append(dict)\n",
    "            except:\n",
    "                print(\"No report found at some point\")\n",
    "\n",
    "component_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "text_dat = {}\n",
    "for component in component_dict:\n",
    "    soup = bs(\n",
    "        req.get(\n",
    "            component[\"url\"],\n",
    "            headers=head\n",
    "        ).text,\n",
    "        \"html\"\n",
    "    )\n",
    "    txtlist = re.split(r' *[\\n\\.\\?!][\\'\"\\)\\]]* *',soup.text.strip())\n",
    "    text_dat[component['name']] = [ txt for txt in txtlist if len(txt) > 100 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2603\n",
      "[[ 7.096506  -4.374116  -4.7448015]]\n"
     ]
    }
   ],
   "source": [
    "txt = \"The Company leases facilities under non-cancellable operating leases primarily in the United States, South Africa, the United Kingdom and Canada. The Company’s operating leases have remaining lease terms of between less than one to 12 years, some of which include options to extend the leases for up to five years, and some of which include options to terminate the leases within one year. These options to extend the terms of the Company’s operating leases were not deemed to be reasonably certain of exercise as of lease commencement and are therefore not included in the determination of their respective non-cancellable lease terms. The future lease payments due under non-cancellable operating lease arrangements contain fixed rent increases over the term of the lease. The Company also leases office equipment under non-cancellable leases.The following table presents the components of lease expense on the Company’s condensed consolidated statements of operations and comprehensive loss for each of the periods indicated.The Company leases facilities under non-cancellable operating leases primarily in the United States, South Africa, the United Kingdom and Canada. The Company’s operating leases have remaining lease terms of between less than one to 12 years, some of which include options to extend the leases for up to five years, and some of which include options to terminate the leases within one year. These options to extend the terms of the Company’s operating leases were not deemed to be reasonably certain of exercise as of lease commencement and are therefore not included in the determination of their respective non-cancellable lease terms. The future lease payments due under non-cancellable operating lease arrangements contain fixed rent increases over the term of the lease. The Company also leases office equipment under non-cancellable leases.The following \\table presents the components of lease expense on the Company’s condensed consolidated statements of operations and comprehensive loss for each of the periods indicated.The Company leases facilities under non-cancellable operating leases primarily in the United States, South Africa, the United Kingdom and Canada. The Company’s operating leases have remaining lease terms of between less than one to 12 years, some of which include options to extend the leases for up to five years, and some of which include options to terminate the leases within one year.lable operating leases primarily in the United States, South Africa, the United Kingdom and Canada. The Company’s operating leases have remaining lease terms\"\n",
    "print(len(txt))\n",
    "x = tokenizer(txt, return_tensors=\"pt\", padding=True)\n",
    "encoded_sentiment = finbert(**x)[0].detach().numpy()\n",
    "print(encoded_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for key,txt_list in text_dat.items():\n",
    "    print(key)\n",
    "    print(\"Example Sentence: \", txt_list[0])\n",
    "\n",
    "    output = {}\n",
    "    output['name'] = key\n",
    "\n",
    "    encoded_text = tokenizer(txt_list, return_tensors=\"pt\", padding=True)\n",
    "    encoded_sentiment = finbert(**encoded_text)[0].detach().numpy()\n",
    "    refined_sentiment = [i for i in encoded_sentiment if np.argmax(i) != 0]\n",
    "    average_sentiment = np.average(refined_sentiment, axis=0)\n",
    "\n",
    "    output['component-sentiment'] = labels[np.argmax(average_sentiment)]\n",
    "\n",
    "    print(\"Overall\", output['component-sentiment'])\n",
    "    output['sentiment-list'] = []\n",
    "    output['values-list'] = []\n",
    "    for i,sentence in enumerate(txt_list):\n",
    "        output['sentiment-list'].append( labels[ np.argmax(  encoded_sentiment[i]  ) ] )\n",
    "        output['values-list'].append(encoded_sentiment[i])\n",
    "\n",
    "\n",
    "    outputs.append(output)\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Calc But in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Availability:  True\n",
      "Current Device :  NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Available Devices\n",
      "0 ) Device Name:  NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Availability: \", torch.cuda.is_available())\n",
    "gpu_index = torch.cuda.current_device()\n",
    "print(\"Current Device : \", torch.cuda.get_device_name(gpu_index))\n",
    "print(\"Available Devices\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, \") Device Name: \", torch.cuda.get_device_name(i))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random testing\n",
    "\n",
    "X_train = torch.FloatTensor([0., 1., 2., 3., 4.])\n",
    "X_train.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common practice to hold the GPU in device\n",
    "X_train = X_train.to(device)\n",
    "X_train.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.21 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kaushik Dey\\Documents\\DigitalAlpha\\python-notebooks\\finbert-analysis.ipynb Cell 22'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000026?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mGPUtil\u001b[39;00m \u001b[39mimport\u001b[39;00m showUtilization \u001b[39mas\u001b[39;00m gpu_usage\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000026?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000026?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfree_gpu_cache\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000026?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitial GPU Usage\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\numba\\__init__.py:200\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=196'>197</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=198'>199</a>\u001b[0m _ensure_llvm()\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=199'>200</a>\u001b[0m _ensure_critical_deps()\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=201'>202</a>\u001b[0m \u001b[39m# we know llvmlite is working as the above tests passed, import it now as SVML\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=202'>203</a>\u001b[0m \u001b[39m# needs to mutate runtime options (sets the `-vector-library`).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=203'>204</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mllvmlite\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\numba\\__init__.py:140\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=137'>138</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumba needs NumPy 1.18 or greater\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=138'>139</a>\u001b[0m \u001b[39melif\u001b[39;00m numpy_version \u001b[39m>\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m21\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=139'>140</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumba needs NumPy 1.21 or less\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=141'>142</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/numba/__init__.py?line=142'>143</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 1.21 or less"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "finbert.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    finbert = finbert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103102 - Disclosure - Summary of Significant Accounting Policies\n",
      "Example Sentence:  Summary of Significant Accounting PoliciesConcentrations of RisksThe Company’s revenue is reliant on its customers utilizing Internet-based services\n",
      "Total Number of Sentences:  168\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:251\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=249'>250</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=250'>251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=251'>252</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cuda'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kaushik Dey\\Documents\\DigitalAlpha\\python-notebooks\\finbert-analysis.ipynb Cell 24'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000025?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000025?line=9'>10</a>\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m key\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000025?line=10'>11</a>\u001b[0m encoded_text \u001b[39m=\u001b[39m tokenizer(txt_list, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000025?line=11'>12</a>\u001b[0m sentiment_tensor \u001b[39m=\u001b[39m finbert(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kaushik%20Dey/Documents/DigitalAlpha/python-notebooks/finbert-analysis.ipynb#ch0000025?line=12'>13</a>\u001b[0m encoded_sentiment \u001b[39m=\u001b[39m sentiment_tensor[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:253\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=250'>251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=251'>252</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=252'>253</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for key,txt_list in text_dat.items():\n",
    "    print(key)\n",
    "    print(\"Example Sentence: \", txt_list[0])\n",
    "    print(\"Total Number of Sentences: \", len(txt_list))\n",
    "    output = {}\n",
    "    output['name'] = key\n",
    "    encoded_text = tokenizer(txt_list, return_tensors=\"pt\", padding=True).cuda()\n",
    "    sentiment_tensor = finbert(**encoded_text)\n",
    "    encoded_sentiment = sentiment_tensor[0].detach().numpy()\n",
    "    refined_sentiment = [i for i in encoded_sentiment if np.argmax(i) != 0]\n",
    "    average_sentiment = np.average(refined_sentiment, axis=0)\n",
    "\n",
    "    output['component-sentiment'] = labels[np.argmax(average_sentiment)]\n",
    "\n",
    "    print(\"Overall\", output['component-sentiment'])\n",
    "    # output['sentiment-list'] = []\n",
    "    # output['values-list'] = []\n",
    "    # for i,sentence in enumerate(txt_list):\n",
    "    #     output['sentiment-list'].append( labels[ np.argmax(  encoded_sentiment[i]  ) ] )\n",
    "    #     output['values-list'].append(encoded_sentiment[i])\n",
    "\n",
    "\n",
    "    outputs.append(output)\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
